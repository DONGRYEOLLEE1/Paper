{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_7a0xJA9GJZ"
      },
      "outputs": [],
      "source": [
        "patch_size = 16\n",
        "patches = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1 = patch_size, s2 = patch_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te0uXBIA9o3O",
        "outputId": "00fed4f1-29db-4aeb-e466-1dfc936e9fb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1+cu116\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "\n",
        "import cv2\n",
        "\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize, CenterCrop\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "from torch import optim\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import utils\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "import copy\n",
        "import time\n",
        "\n",
        "\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chR_JMFb9pjx"
      },
      "outputs": [],
      "source": [
        "# PatchEmbedding Ver1\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels = 3, patch_size = 16, emb_size = 768):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(Rearrange('b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1 = patch_size, s2 = patch_size),\n",
        "                                        nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
        "                                        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.projection(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "# PatchEmbedding Ver2\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels = 3, patch_size = 16, emb_size = 768):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(nn.Conv2d(in_channels, emb_size, kernel_size = patch_size, stride = patch_size),\n",
        "                                        Rearrange('b e (h) (w) -> b (h w) e')\n",
        "                                        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.projection(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "# CLS Token\n",
        "\"\"\"\n",
        "PE를 위한 'cls token' 추가, 각 시퀀스 앞에 붙여주기\n",
        "\"\"\"\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels = 3, patch_size = 16, emb_size = 768):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size = patch_size, stride = patch_size),\n",
        "            Rearrange('b e (h) (w) -> b (h w ) e')\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        \n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.projection(x)\n",
        "        cls_token = repeat(self.cls_token, '() n e -> b n e', b = b)\n",
        "        \n",
        "        x = torch.cat([cls_token, x], dim = 1)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "# Position Embedding\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels = 3, patch_size = 16, emb_size = 768, img_size = 224):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size = patch_size, stride = patch_size),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e')\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))\n",
        "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) ** 2 + 1, emb_size))\n",
        "        \n",
        "    def forward(self, x : Tensor) -> Tensor:\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.projection(x)\n",
        "        cls_token = repeat(self.cls_token, '() n e -> b n e', b = b)\n",
        "        \n",
        "        # Input cls_token\n",
        "        x = torch.cat([cls_token, x], dim = 1)\n",
        "        \n",
        "        # add Position Embedding\n",
        "        x += self.positions\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDfmjhVG9qNU"
      },
      "outputs": [],
      "source": [
        "# MultiHeadAttention\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size = 768, num_heads = 8, dropout = 0):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.keys = nn.Linear(emb_size, emb_size)\n",
        "        self.queries = nn.Linear(emb_size, emb_size)\n",
        "        self.values = nn.Linear(emb_size, emb_size)\n",
        "        \n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "        self.scaling = (self.emb_size // self.num_heads) ** -0.5\n",
        "        \n",
        "    def forward(self, x : Tensor, mask : Tensor = None) -> Tensor:\n",
        "        # split q, k, v in num_heads\n",
        "        queries = rearrange(self.queries(x), 'b n (h d) -> b h n d', h = self.num_heads)\n",
        "        keys = rearrange(self.keys(x), 'b n (h d) -> b h n d', h = self.num_heads)\n",
        "        values = rearrange(self.values(x), 'b n (h d) -> b h n d', h = self.num_heads)\n",
        "        \n",
        "        # sum\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)  ## batch, num_heads, query_len, key_len\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "            \n",
        "        att = F.softmax(energy, dim = -1) * self.scaling\n",
        "        att = self.att_drop(att)\n",
        "        \n",
        "        # sum up over the third axis\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        out = self.projection(out)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEdpqVVP9qQX"
      },
      "outputs": [],
      "source": [
        "# Residuals\n",
        "\n",
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        \n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        \n",
        "        return x\n",
        "# MLP\n",
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size, expansion = 4, drop_p = 0):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KMw9gVi9qTX"
      },
      "outputs": [],
      "source": [
        "# Transformer\n",
        "\n",
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size = 768, drop_p = 0, forward_expansion = 4, forward_drop_p = 0, **kwargs):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, **kwargs),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(emb_size, expansion = forward_expansion, drop_p = forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            ))\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xADdGlcW9qWQ"
      },
      "outputs": [],
      "source": [
        "# Transformer\n",
        "\"\"\"\n",
        "ViT에서는 original Transformer의 encoder 부분만을 사용함.\n",
        "encoder는 TransformerBlock의 L block.\n",
        "\n",
        "ViT-Base : 12\n",
        "ViT-Large : 24\n",
        "ViT-Huge : 32\n",
        "\"\"\"\n",
        "\n",
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth = 12, **kwargs):\n",
        "        super().__init__( *[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlbxSfK59qYm"
      },
      "outputs": [],
      "source": [
        "# Head\n",
        "\"\"\"\n",
        "마지막 레이어는 noraml FC (레이블에 대한 확률 값)\n",
        "\"\"\"\n",
        "\n",
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size = 768, n_classes = 1000):\n",
        "        super().__init__(\n",
        "            Reduce('b n e -> b e', reduction = 'mean'),\n",
        "            nn.LayerNorm(emb_size),\n",
        "            nn.Linear(emb_size, n_classes)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNo1RSJH9qbC"
      },
      "outputs": [],
      "source": [
        "# ViT (PatchEmbedding + TransformerEncoder + ClassificationHead)\n",
        "\n",
        "class ViT(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 in_channels = 3,\n",
        "                 patch_size = 16,\n",
        "                 emb_size = 768,\n",
        "                 img_size = 224,\n",
        "                 depth = 12,\n",
        "                 n_classes = 1000,\n",
        "                 **kwargs):\n",
        "        super().__init__(\n",
        "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
        "            TransformerEncoder(depth, emb_size = emb_size, **kwargs),\n",
        "            ClassificationHead(emb_size, n_classes)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEf502vC9qd_"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141,
          "referenced_widgets": [
            "870dca0999c44b2da4f58a128cedcc4f",
            "bf2307bd08d043f6a92a262b0d0de27c",
            "cf02f762a8ad4e6c857c17645035f4d9",
            "69d84795d53146f590b833609c7f4961",
            "adfd023bd8d240f5801d627a8b0b1e66",
            "cc6cdea7cafc42018be1153a5295efb7",
            "9dc6b16abb87439ea96982f3479e6dfe",
            "1c615ff4d05342c9a3f1d27ea93b1b43",
            "c071c40f1ee348348ac9e916c79d2d9e",
            "10e7ea6d32cf483f83dddbd74e1904b6",
            "5247b2bd5ff84733bdba100584ceb397"
          ]
        },
        "id": "A2zDuWZr-H9R",
        "outputId": "896ead68-82ab-47d8-b2fb-ec174a004803"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://ai.stanford.edu/~acoates/stl10/stl10_binary.tar.gz to ./data/stl10_binary.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "870dca0999c44b2da4f58a128cedcc4f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2640397119 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/stl10_binary.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "5000\n",
            "8000\n"
          ]
        }
      ],
      "source": [
        "## 데이터셋\n",
        "\n",
        "data_path = './data'\n",
        "\n",
        "# load\n",
        "train_ds = datasets.STL10(data_path, split = 'train', download = True, transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(224)\n",
        "]))\n",
        "val_ds = datasets.STL10(data_path, split = 'test', download = True, transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize(224)\n",
        "]))\n",
        "\n",
        "print(len(train_ds))\n",
        "print(len(val_ds))\n",
        "train_dl = DataLoader(train_ds, batch_size = 32, shuffle = True)\n",
        "val_dl = DataLoader(val_ds, batch_size = 64, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCooYbRf-IBM",
        "outputId": "8215fd59-97d4-45e5-f9fe-2b75dbc63fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 197, 768])\n",
            "torch.Size([16, 197, 768])\n",
            "torch.Size([16, 1, 128])\n",
            "torch.Size([16, 197, 768])\n",
            "torch.Size([16, 197, 768])\n",
            "torch.Size([16, 1000])\n",
            "torch.Size([16, 1000])\n"
          ]
        }
      ],
      "source": [
        "x = torch.randn(16, 3, 224, 224).to(device)\n",
        "\n",
        "patch_embedding = PatchEmbedding().to(device)\n",
        "patch_output = patch_embedding(x)\n",
        "print(patch_output.shape)\n",
        "\n",
        "MHA = MultiHeadAttention().to(device)\n",
        "MHA_output = MHA(patch_output)\n",
        "print(MHA_output.shape)\n",
        "\n",
        "x = torch.randn(16, 1, 128).to(device)\n",
        "\n",
        "model = FeedForwardBlock(128).to(device)\n",
        "output = model(x)\n",
        "print(output.shape)\n",
        "\n",
        "model = TransformerEncoderBlock().to(device)\n",
        "output = model(patch_output)\n",
        "print(output.shape)\n",
        "\n",
        "model = TransformerEncoder().to(device)\n",
        "output = model(patch_output)\n",
        "print(output.shape)\n",
        "\n",
        "x = torch.randn(16, 1, 768).to(device)\n",
        "model = ClassificationHead().to(device)\n",
        "output = model(x)\n",
        "print(output.shape)\n",
        "\n",
        "x = torch.randn(16,3,224,224).to(device)\n",
        "model = ViT().to(device)\n",
        "output = model(x)\n",
        "print(output.shape)\n",
        "\n",
        "model = ViT().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-55VZqi-IEF"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "loss_fn = nn.CrossEntropyLoss(reduction = 'sum')\n",
        "optimizer = optim.Adam(model.parameters(), lr = .01)\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(optimizer, mode = 'min', factor = .1, patience = 10)\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "def metric_batch(output, target):\n",
        "    pred = output.argmax(1, keepdim = True)\n",
        "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
        "    return corrects\n",
        "\n",
        "def loss_batch(loss_fn, output, target, optimizer = None):\n",
        "    loss_b = loss_fn(output, target)\n",
        "    metric_b = metric_batch(output, target)\n",
        "    \n",
        "    if optimizer is not None:\n",
        "        optimizer.zero_grad()\n",
        "        loss_b.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    return loss_b.item(), metric_b\n",
        "    \n",
        "def loss_epoch(model, loss_fn, dataset_dl, sanity_check = False, optimizer = None):\n",
        "    running_loss = 0.0\n",
        "    running_metric = 0.0\n",
        "    len_data = len(dataset_dl.dataset)\n",
        "    \n",
        "    for xb, yb in dataset_dl:\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        output = model(xb)\n",
        "        \n",
        "        loss_b, metric_b = loss_batch(loss_fn, output, yb, optimizer)\n",
        "        running_loss += loss_b\n",
        "        \n",
        "        if metric_b is not None:\n",
        "            running_metric += metric_b\n",
        "        if sanity_check is True:\n",
        "            break\n",
        "        \n",
        "    loss = running_loss / len_data\n",
        "    metric = running_metric / len_data\n",
        "    return loss, metric\n",
        "def train_val(model, params):\n",
        "    num_epochs=params['num_epochs']\n",
        "    loss_func=params['loss_func']\n",
        "    opt=params['optimizer']\n",
        "    train_dl=params['train_dl']\n",
        "    val_dl=params['val_dl']\n",
        "    sanity_check=params['sanity_check']\n",
        "    lr_scheduler=params['lr_scheduler']\n",
        "    path2weights=params['path2weights']\n",
        "\n",
        "    loss_history = {'train': [], 'val': []}\n",
        "    metric_history = {'train': [], 'val': []}\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        current_lr = get_lr(opt)\n",
        "        print('Epoch {}/{}, current lr= {}'.format(epoch, num_epochs-1, current_lr))\n",
        "\n",
        "        model.train()\n",
        "        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
        "        loss_history['train'].append(train_loss)\n",
        "        metric_history['train'].append(train_metric)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
        "        loss_history['val'].append(val_loss)\n",
        "        metric_history['val'].append(val_metric)\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(model.state_dict(), path2weights)\n",
        "            print('Copied best model weights!')\n",
        "\n",
        "        lr_scheduler.step(val_loss)\n",
        "        if current_lr != get_lr(opt):\n",
        "            print('Loading best model weights!')\n",
        "            model.load_state_dict(best_model_wts)\n",
        "\n",
        "        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n",
        "        print('-'*10)\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, loss_history, metric_history\n",
        "params_train = {\n",
        "    'num_epochs':20,\n",
        "    'optimizer':optimizer,\n",
        "    'loss_func':loss_fn,\n",
        "    'train_dl':train_dl,\n",
        "    'val_dl':val_dl,\n",
        "    'sanity_check':False,\n",
        "    'lr_scheduler':lr_scheduler,\n",
        "    'path2weights':'./models/weights.pt',\n",
        "}\n",
        "\n",
        "# check the directory to save weights.pt\n",
        "def createFolder(directory):\n",
        "    try:\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "    except OSerror:\n",
        "        print('Error')\n",
        "createFolder('./models')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uijifLZS-IG1",
        "outputId": "23840a2d-65df-43f1-9dcd-040547f24918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0/19, current lr= 0.01\n",
            "Copied best model weights!\n",
            "train loss: 2.912359, val loss: 2.375173, accuracy: 14.84, time: 4.3038 min\n",
            "----------\n",
            "Epoch 1/19, current lr= 0.01\n",
            "Copied best model weights!\n",
            "train loss: 2.224996, val loss: 2.251253, accuracy: 14.22, time: 8.6805 min\n",
            "----------\n",
            "Epoch 2/19, current lr= 0.01\n",
            "Copied best model weights!\n",
            "train loss: 2.100978, val loss: 2.048102, accuracy: 16.18, time: 13.0611 min\n",
            "----------\n",
            "Epoch 3/19, current lr= 0.01\n",
            "Copied best model weights!\n",
            "train loss: 2.014524, val loss: 2.013837, accuracy: 18.21, time: 17.4371 min\n",
            "----------\n",
            "Epoch 4/19, current lr= 0.01\n",
            "Copied best model weights!\n",
            "train loss: 2.016816, val loss: 2.010707, accuracy: 19.61, time: 21.8034 min\n",
            "----------\n",
            "Epoch 5/19, current lr= 0.01\n",
            "Copied best model weights!\n",
            "train loss: 1.997171, val loss: 1.992174, accuracy: 20.44, time: 26.1699 min\n",
            "----------\n",
            "Epoch 6/19, current lr= 0.01\n",
            "Copied best model weights!\n",
            "train loss: 1.987082, val loss: 1.966063, accuracy: 21.76, time: 30.5523 min\n",
            "----------\n",
            "Epoch 7/19, current lr= 0.01\n",
            "Copied best model weights!\n",
            "train loss: 1.958335, val loss: 1.911348, accuracy: 24.68, time: 34.9378 min\n",
            "----------\n",
            "Epoch 8/19, current lr= 0.01\n",
            "train loss: 1.984902, val loss: 2.100987, accuracy: 17.89, time: 39.2825 min\n",
            "----------\n",
            "Epoch 9/19, current lr= 0.01\n",
            "train loss: 1.962041, val loss: 1.986043, accuracy: 22.21, time: 43.6302 min\n",
            "----------\n",
            "Epoch 10/19, current lr= 0.01\n",
            "train loss: 1.968787, val loss: 1.936086, accuracy: 21.81, time: 47.9737 min\n",
            "----------\n",
            "Epoch 11/19, current lr= 0.01\n",
            "train loss: 1.907871, val loss: 1.977195, accuracy: 24.56, time: 52.3068 min\n",
            "----------\n",
            "Epoch 12/19, current lr= 0.01\n",
            "Copied best model weights!\n",
            "train loss: 1.899952, val loss: 1.903037, accuracy: 26.85, time: 56.6601 min\n",
            "----------\n",
            "Epoch 13/19, current lr= 0.01\n",
            "Copied best model weights!\n",
            "train loss: 1.892509, val loss: 1.883789, accuracy: 26.81, time: 61.0001 min\n",
            "----------\n",
            "Epoch 14/19, current lr= 0.01\n",
            "Copied best model weights!\n",
            "train loss: 1.867185, val loss: 1.859845, accuracy: 28.01, time: 65.3284 min\n",
            "----------\n",
            "Epoch 15/19, current lr= 0.01\n",
            "train loss: 1.849679, val loss: 1.903149, accuracy: 25.64, time: 69.6331 min\n",
            "----------\n",
            "Epoch 16/19, current lr= 0.01\n",
            "train loss: 2.008373, val loss: 1.973479, accuracy: 26.35, time: 73.9181 min\n",
            "----------\n",
            "Epoch 17/19, current lr= 0.01\n",
            "Copied best model weights!\n",
            "train loss: 1.905460, val loss: 1.851627, accuracy: 26.80, time: 78.2192 min\n",
            "----------\n",
            "Epoch 18/19, current lr= 0.01\n",
            "train loss: 1.846508, val loss: 1.950571, accuracy: 21.18, time: 82.4866 min\n",
            "----------\n",
            "Epoch 19/19, current lr= 0.01\n",
            "train loss: 1.931415, val loss: 1.941118, accuracy: 25.52, time: 86.7295 min\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "model, loss_hist, metric_hist = train_val(model, params_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ab3pWZ5r-IJT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10e7ea6d32cf483f83dddbd74e1904b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c615ff4d05342c9a3f1d27ea93b1b43": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5247b2bd5ff84733bdba100584ceb397": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69d84795d53146f590b833609c7f4961": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10e7ea6d32cf483f83dddbd74e1904b6",
            "placeholder": "​",
            "style": "IPY_MODEL_5247b2bd5ff84733bdba100584ceb397",
            "value": " 2640397119/2640397119 [03:05&lt;00:00, 12105691.09it/s]"
          }
        },
        "870dca0999c44b2da4f58a128cedcc4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf2307bd08d043f6a92a262b0d0de27c",
              "IPY_MODEL_cf02f762a8ad4e6c857c17645035f4d9",
              "IPY_MODEL_69d84795d53146f590b833609c7f4961"
            ],
            "layout": "IPY_MODEL_adfd023bd8d240f5801d627a8b0b1e66"
          }
        },
        "9dc6b16abb87439ea96982f3479e6dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adfd023bd8d240f5801d627a8b0b1e66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf2307bd08d043f6a92a262b0d0de27c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc6cdea7cafc42018be1153a5295efb7",
            "placeholder": "​",
            "style": "IPY_MODEL_9dc6b16abb87439ea96982f3479e6dfe",
            "value": "100%"
          }
        },
        "c071c40f1ee348348ac9e916c79d2d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc6cdea7cafc42018be1153a5295efb7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf02f762a8ad4e6c857c17645035f4d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c615ff4d05342c9a3f1d27ea93b1b43",
            "max": 2640397119,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c071c40f1ee348348ac9e916c79d2d9e",
            "value": 2640397119
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}