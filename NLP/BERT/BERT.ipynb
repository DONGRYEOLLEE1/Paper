{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "b01GfqJcyRdC"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 1.x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMa5Jhjh0LLb",
        "outputId": "5878bf60-ea69-4753-c7fd-48da57d07707"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mesh-tensorflow               0.1.12\n",
            "tensorflow                    1.15.2\n",
            "tensorflow-datasets           4.0.1\n",
            "tensorflow-estimator          1.15.1\n",
            "tensorflow-gan                2.0.0\n",
            "tensorflow-gcs-config         2.8.0\n",
            "tensorflow-hub                0.12.0\n",
            "tensorflow-io-gcs-filesystem  0.24.0\n",
            "tensorflow-metadata           1.6.0\n",
            "tensorflow-probability        0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFpPFPFTyUxI",
        "outputId": "0323bc5f-938a-4637-f42b-f92066415fe1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir \"/content/gdrive/MyDrive/Colab Notebooks/bert-pretrain-v2\"\n",
        "%cd \"/content/gdrive/MyDrive/Colab Notebooks/bert-pretrain-v2\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9XJxxPcyU4-",
        "outputId": "f5b4df81-d218-4df2-96e3-8a997b93fb4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/Colab Notebooks/bert-pretrain-v2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KcBERT Dataset\n",
        "- \"공개된 한국어 BERT는 대부분 한국어 위키, 뉴스 기사, 책 등 잘 정제된 데이터를 기반으로 학습한 모델입니다. 한편, 실제로 NSMC와 같은 댓글형 데이터셋은 정제되지 않았고 구어체 특징에 신조어가 많으며, 오탈자 등 공식적인 글쓰기에서 나타나지 않는 표현들이 빈번하게 등장합니다.KcBERT는 위와 같은 특성의 데이터셋에 적용하기 위해, 네이버 뉴스에서 댓글과 대댓글을 수집해, 토크나이저와 BERT모델을 처음부터 학습한 Pretrained BERT 모델"
      ],
      "metadata": {
        "id": "T56lqgUHzdje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q Korpora\n",
        "!pip install -q tokenizers\n",
        "!git clone -q https://github.com/google-research/bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFwJKAlJyU5l",
        "outputId": "43389b28-8694-49e0-ff60-b5815732ff07"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 57 kB 2.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 96 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 5.5 MB/s \n",
            "\u001b[?25hfatal: destination path 'bert' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 필요 모듈 불러오기"
      ],
      "metadata": {
        "id": "u-1LP0o7zuTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import auth, drive # GCS를 코랩과 연동\n",
        "\n",
        "# BERT 경로 추가\n",
        "sys.path.append(\"bert\")\n",
        "\n",
        "from bert import modeling, optimization, tokenization\n",
        "from bert.run_pretraining import input_fn_builder, model_fn_builder"
      ],
      "metadata": {
        "id": "T6eOCy0AzqJL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbIUjZY0yU6S",
        "outputId": "b382fa2a-6c73-4482-ee89-520f1fc907dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TPU 설정"
      ],
      "metadata": {
        "id": "LbJTa10l2MK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# log 정의\n",
        "logger = logging.getLogger('tensorflow')\n",
        "logger.setLevel(logging.INFO)\n",
        "# log format 정의\n",
        "formatter = logging.Formatter('%(asctime)s : %(message)s')\n",
        "sh = logging.StreamHandler()\n",
        "sh.setFormatter(formatter)\n",
        "logger.handlers = [sh]\n",
        "\n",
        "# TPU 설정\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    logger.info('Using TPU runtime')\n",
        "    USE_TPU = True \n",
        "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    # 세션이 변경되면 tpu 주소가 바뀌기 때문에 설정 \n",
        "    \n",
        "    with tf.Session(TPU_ADDRESS) as session: \n",
        "        logger.info('TPU address is ' + TPU_ADDRESS) \n",
        "        with open('/content/adc.json', 'r') as f: \n",
        "            # GCS /content/abc.json에 설정한 과정이 저장되고 이 설정파일로 TPU 사용 \n",
        "            auth_info = json.load(f) \n",
        "        tf.contrib.cloud.configure_gcs(session, credentials=auth_info) \n",
        "        \n",
        "else:\n",
        "    raise Exception(\"런타임 -> 런타임 유형변경 -> TPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xOgAMCCyU6_",
        "outputId": "c018465f-fad2-4408-9639-a1e87517f9f8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-02-23 13:49:57,329 : Using TPU runtime\n",
            "2022-02-23 13:49:57,332 : TPU address is grpc://10.106.153.218:8470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 불러오기"
      ],
      "metadata": {
        "id": "HRkVnbev2KRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Korpora import Korpora\n",
        "\n",
        "Korpora.fetch('kcbert', root_dir = '.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRZZU85QyU78",
        "outputId": "7d2dadbb-0310-4c3b-f1b4-9fa1e8239c33"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[kcbert] download kcbert-train.tar.gzaa: 100%|██████████| 2.10G/2.10G [00:41<00:00, 51.0MB/s]\n",
            "[kcbert] download kcbert-train.tar.gzab: 100%|██████████| 2.10G/2.10G [01:36<00:00, 21.8MB/s]\n",
            "[kcbert] download kcbert-train.tar.gzac: 671MB [00:08, 75.0MB/s]                           \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unzip tar. It needs a few minutes ... done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 확인\n",
        "!ls kcbert/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHvsIc7LyU8w",
        "outputId": "9db7c98a-5bac-4eee-c759-94af63b15bf5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20190101_20200611_v2.txt  kcbert-train.tar.gzab\n",
            "kcbert-train.tar.gzaa\t  kcbert-train.tar.gzac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리가 완료된 데이터\n",
        "\n",
        "check_data = []\n",
        "\n",
        "with open('kcbert/20190101_20200611_v2.txt', 'r') as f:\n",
        "    for i in range(10):\n",
        "        check_data.append(f.readline())\n",
        "\n",
        "check_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7vrNaX3yU9q",
        "outputId": "58931ac3-ef42-4dd2-be56-f1dc39120815"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['우리에게 북한은 꼭 없애야 할 적일뿐\\n',\n",
              " '문재앙 탄핵 원하면 추천 눌러주세요!! 여론의 힘을 보여줍시다\\n',\n",
              " '김정은이 트럼프를 개호구 문재인으로 착각했나봄ㅋㅋ\\n',\n",
              " '부칸이 밑장빼기하다가 딱 걸렸군\\n',\n",
              " '북한 욕하면 친일이란 그딴 사고방식은 김일성 종합대학에서 배웠니? ㅋㅋ\\n',\n",
              " '처음부터 북한은 비핵화할 생각이 없다고 보았는데\\n',\n",
              " 'ㅋㅋㅋ 북한 핵돼지 60시간 넘게 개고생하면서 기차 타고 왔는데 트럼프가 빅엿 날리네ㅋㅋㅋ\\n',\n",
              " '문모씨의 북한팔이도 이제 끝이네요\\n',\n",
              " '뻔하지머. 트럼프: 비핵화 플랜 내놔라. 김정은: 우선 경제 제재부터 풀어달라. 끝~\\n',\n",
              " '문재인 작살났네. 개성공단 개박살났네. 금강산 불났네. 남북철도연결은 폭탄맞았네.\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vocab 생성\n",
        "- 100만개만 학습시켜보자"
      ],
      "metadata": {
        "id": "_xkH8Tbs2oih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEMO_MODE = True\n",
        "\n",
        "if DEMO_MODE:\n",
        "    CORPUS_SIZE = 1000000\n",
        "    !(head -n $CORPUS_SIZE kcbert/20190101_20200611_v2.txt) > dataset.txt\n",
        "else:\n",
        "    !mv kcbert/20190101_20200611_v2.txt dataset.txt"
      ],
      "metadata": {
        "id": "zG4ixcY-yU-T"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh dataset.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iHXVAH1yU-3",
        "outputId": "fafaba15-dc4a-4265-be4f-662cfe99209f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw------- 1 root root 144M Feb 23 13:58 dataset.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 데이터셋 나누기\n",
        "- 4개의 파일에 25만개씩 나누기\n",
        "\n",
        "- `wordpiece tokenizer`를 통해 vocab을 생성, vocab사이즈는 약 1만개, 단일 글자 최대는 3000개, bert문자는 4개로 생성\n",
        "\n",
        "- `BertWordPieceTokenizer` 파라미터에서 한국어를 사용할 때, **lowercase = False**를 지정해주는데, 이와 동시에 **strip_accents = False**로 지정해야함. `ㄱ`, `ㅡ`, `ㄴ` 와 같이 단어가 분리된 형태까지 학습되어서 토크나이저 학습이 잘 이루어지지 않는다."
      ],
      "metadata": {
        "id": "JCbKmFb73lE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./shards\n",
        "!split -a 4 -l 256000 -d dataset.txt ./shards/shard_\n",
        "!ls ./shards/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVwn-IaEyU_c",
        "outputId": "3362b7d2-25a0-430a-bfa4-6026dea0cd82"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shard_0000  shard_0001\tshard_0002  shard_0003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# huggingface의 Tokenizer 라이브러리로 Tokenizer 생성\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "tokenizer = BertWordPieceTokenizer(lowercase = False, strip_accents = False)\n",
        "tokenizer.train(['dataset.txt'], vocab_size = 13004, limit_alphabet = 3000)"
      ],
      "metadata": {
        "id": "xOqVwKLgyVAE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_model('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jAvVdhNyVAp",
        "outputId": "812b2ab7-b59e-45e5-cc3c-6e73ce112279"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- vocab을 생성할 때, wordpiece, sentencepiece, 형태소분석기 등을 사용할 수 있고, vocab을 생성한 뒤,  Tokenizer로 텍스트를 자른다. \n",
        "\n",
        "- 학습된 vocab을 BERT가 이해하려면 `_`로 시작한 토큰을 `##`로 바꿔주고 `[PAD]`, `[UNK]`, `[CLS]`, `[SEP]`, `[MASK]`와 같은 BERT에서 사용하는 특수 토큰 정보를 추가해주어야한다. 이를 `BertWordPieceTokenizer`가 해준뒤 생성된 토큰을 한줄 한줄 저장한 vocab.txt에 저장한다."
      ],
      "metadata": {
        "id": "_ZjJSLHa41e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 20 vocab.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLX2TLWdyVBT",
        "outputId": "ef48fb31-1843-47b6-a629-073cad4ee638"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "우익\n",
            "주머니\n",
            "프랑\n",
            "##공립\n",
            "##키스탄\n",
            "반대한다\n",
            "정도껏\n",
            "행동은\n",
            "다니고\n",
            "네이버는\n",
            "알아야지\n",
            "저러니\n",
            "오르면\n",
            "소리는\n",
            "자진\n",
            "쪽발\n",
            "철면피\n",
            "하던지\n",
            "##미세먼\n",
            "##가격\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BERT 파라미터 설정"
      ],
      "metadata": {
        "id": "BCtIryUz5QR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- MAX_SEQ_LENGTH: BERT의 모델 입력의 최장 토큰 길이 이 이상으로는 BERT모델이 이해하지 못한다.\n",
        "- MASKED_LM_PROB: BERT의 학습 중 Masked LM의 비율을 조정한다.\n",
        "- MAX_PREDICTIONS: Sequence별 예측할 최대 숫자\n",
        "- DO_LOWER_CASE: 영문자를 lower(소문자화) 할 지.\n",
        "- PROCESSES: 전처리할때 CPU 몇개 쓸지\n",
        "- PRETRAINING_DIR: 프리트레인 데이터 폴더 이름\n",
        "- VOC_FNAME: 방금 만들어준 vocab.txt 위치"
      ],
      "metadata": {
        "id": "7DPKXlh55ood"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 128\n",
        "MASKED_LM_PROB = 0.15\n",
        "MAX_PREDICTIONS = 20\n",
        "DO_LOWER_CASE = False\n",
        "PROCESSES = 4\n",
        "PRETRAINING_DIR = \"pretraining_data\"\n",
        "VOC_FNAME = 'vocab.txt'"
      ],
      "metadata": {
        "id": "K0VTuf_UyVCt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XARGS_CMD = (\"ls ./shards/ | \" \"xargs -n 1 -P {} -I{} \" \"python3 bert/create_pretraining_data.py \" \"--input_file=./shards/{} \" \"--output_file={}/{}.tfrecord \" \"--vocab_file={} \" \"--do_lower_case={} \" \"--max_predictions_per_seq={} \" \"--max_seq_length={} \" \"--masked_lm_prob={} \" \"--random_seed=34 \" \"--dupe_factor=5\") \n",
        "\n",
        "XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}', PRETRAINING_DIR, '{}', VOC_FNAME, DO_LOWER_CASE, MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB)\n",
        "\n",
        "tf.gfile.MkDir(PRETRAINING_DIR)\n",
        "!$XARGS_CMD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ulWUR5uyVDb",
        "outputId": "4e6fcc2c-bd0c-42e5-94e8-b090fae1b3f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From bert/create_pretraining_data.py:469: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0223 14:06:40.538288 140427703019392 module_wrapper.py:139] From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0223 14:06:40.538604 140427703019392 module_wrapper.py:139] From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/Colab Notebooks/bert-pretrain-v2/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0223 14:06:40.538906 140427703019392 module_wrapper.py:139] From /content/gdrive/MyDrive/Colab Notebooks/bert-pretrain-v2/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:469: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:469: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0223 14:06:40.572365 139939739158400 module_wrapper.py:139] From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0223 14:06:40.572698 139939739158400 module_wrapper.py:139] From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/Colab Notebooks/bert-pretrain-v2/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0223 14:06:40.572990 139939739158400 module_wrapper.py:139] From /content/gdrive/MyDrive/Colab Notebooks/bert-pretrain-v2/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0223 14:06:40.573244 139987368757120 module_wrapper.py:139] From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0223 14:06:40.573549 139987368757120 module_wrapper.py:139] From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/Colab Notebooks/bert-pretrain-v2/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0223 14:06:40.573844 139987368757120 module_wrapper.py:139] From /content/gdrive/MyDrive/Colab Notebooks/bert-pretrain-v2/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:469: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0223 14:06:40.580376 140038913484672 module_wrapper.py:139] From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "WARNING:tensorflow:From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0223 14:06:40.580628 140038913484672 module_wrapper.py:139] From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gdrive/MyDrive/Colab Notebooks/bert-pretrain-v2/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0223 14:06:40.580894 140038913484672 module_wrapper.py:139] From /content/gdrive/MyDrive/Colab Notebooks/bert-pretrain-v2/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUCKET_NAME = \"자신의 버킷명 입력\"\n",
        "MODEL_DIR = \"bert_model\"\n",
        "tf.gfile.MkDir(MODEL_DIR)\n",
        "\n",
        "assert BUCKET_NAME != ''\n",
        "\n",
        "출처: https://ebbnflow.tistory.com/162 [Dev Log : 삶은 확률의 구름]"
      ],
      "metadata": {
        "id": "MVZKJ-FHyVEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cBEVjpRNyVEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3uiCrlQmyVFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AYRH8MoqyVF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Vjc9ZjgqyVHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xh1q2FSUyVH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "R0n-4KcWyVIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gfV3j3EZyVJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_FtJJvOcyVJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Zxgl6rePyVKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ah4AqHivyVLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XeIOr2mryUB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DUpljj7gyUQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QC73PlFQyUYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aP6YcgxeyUYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OFkQleqpyUel"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}