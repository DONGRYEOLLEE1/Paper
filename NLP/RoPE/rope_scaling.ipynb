{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Euy2GExlq_uV",
        "outputId": "97aedef1-de1d-49a2-9eb6-82cede20b4b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "79409b5bc7574ad69eb57442d1a37fe5",
            "3139f4c7ad9341a9ad6c67ef796b57d5",
            "b74026602772438399311f1334c74481",
            "c4a67ec132524d30b29a5118f40ce69d",
            "fe016b72ea51457cae37e3059434e555",
            "a8c52246dfcf4434a1529e51a6298e49",
            "560b16e6f4ec4f4c85cae88c4fac1f31",
            "857e276b44804fd09d43f8f71990ce06",
            "10b12751673740109b0f9c5b38c870db",
            "289f6bb171a64da68411830c7be7fb2d",
            "71b3e158d0764e9ba50dcbadbe736c7c"
          ]
        },
        "id": "k6bMGGxNqfrw",
        "outputId": "b2a352cf-6576-4cd3-8bc4-02437ddc52a5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79409b5bc7574ad69eb57442d1a37fe5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"huggyllama/llama-7b\",\n",
        "    load_in_8bit=True,\n",
        "    device_map = 'auto',\n",
        "    rope_scaling={\"type\": \"dynamic\", \"factor\": 2.0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lJnCtzdHqua9"
      },
      "outputs": [],
      "source": [
        "prompt = '''\n",
        "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
        "\n",
        "=== BEGIN ===\n",
        "\n",
        "2306.15595v2 [cs.CL] 28 Jun 2023\n",
        "\n",
        "arXiv\n",
        "\n",
        "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
        "GUAGE MODELS VIA POSITION INTERPOLATION\n",
        "\n",
        "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
        "Meta Platforms Inc.\n",
        "{chenshouyuan, shermanwong, cli, yuandong}@meta . com\n",
        "\n",
        "1 INTRODUCTION\n",
        "\n",
        "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
        "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
        "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
        "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
        "longer context windows are preferred. However, training an LLM from scratch with long context\n",
        "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
        "context window of an existing pre-trained LLM?\n",
        "\n",
        "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
        "text window. However, empirically, we found that models trained this way adapt to long context\n",
        "windows very slowly. After training for more than 10000 batches, the effective context window\n",
        "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
        "inefficient for extending to substantially longer context windows.\n",
        "\n",
        "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
        "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
        "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
        "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
        "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
        "\n",
        "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
        "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
        "down-scale the position indices so that the maximum position index matches the previous context\n",
        "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
        "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
        "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
        "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
        "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
        "\n",
        "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
        "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
        "\n",
        "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
        "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
        "We present experimental results for extending the context window to up to 32768 from the initial\n",
        "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
        "\n",
        "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
        "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
        "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
        "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
        "encodings.\n",
        "\n",
        "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
        "tended context window. We show that models extended by Position Interpolation enjoy\n",
        "significant perplexity gains from greatly extended context windows for text modeling, and\n",
        "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
        "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
        "competitive performances.\n",
        "\n",
        "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
        "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
        "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
        "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
        "a 2048 token limit.\n",
        "\n",
        "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
        "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
        "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
        "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
        "\n",
        "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
        "codings instead.\n",
        "\n",
        "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
        "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
        "window from 2K to 8K. Recently, open source community picks it up in Reddit post ! and Github\n",
        "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
        "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
        "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
        "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
        "extrapolated ones.\n",
        "\n",
        "=== END OF FILE ===\n",
        "\n",
        "'''\n",
        "question = \"Question: What is the paper about?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSwJoKhrqudt",
        "outputId": "23bb5f33-17c5-448d-e274-3cee3016ffbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1516])\n",
            "<s> \n",
            "You are given this machine learning research paper, please read it carefully and answer the follow up question.\n",
            "\n",
            "=== BEGIN ===\n",
            "\n",
            "2306.15595v2 [cs.CL] 28 Jun 2023\n",
            "\n",
            "arXiv\n",
            "\n",
            "EXTENDING CONTEXT WINDOW OF LARGE LAN-\n",
            "GUAGE MODELS VIA POSITION INTERPOLATION\n",
            "\n",
            "Shouyuan Chen Sherman Wong Liangjian Chen  Yuandong Tian\n",
            "Meta Platforms Inc.\n",
            "{chenshouyuan, shermanwong, cli, yuandong}@meta . com\n",
            "\n",
            "1 INTRODUCTION\n",
            "\n",
            "Large language models (LLMs) typically come with a pre-defined context window size. For exam-\n",
            "ple, inputs to LLaMA models (Touvron et al., 2023) must be fewer than 2048 tokens. This pre-set\n",
            "context window limit is frequently exceeded in applications such as conducting long conversations,\n",
            "summarizing long documents, or executing long-term planning. For these applications, LLMs with\n",
            "longer context windows are preferred. However, training an LLM from scratch with long context\n",
            "windows requires significant investments. This naturally leads to a question: Can we extend the\n",
            "context window of an existing pre-trained LLM?\n",
            "\n",
            "One straightforward approach is to fine-tune an existing pre-trained Transformer with a longer con-\n",
            "text window. However, empirically, we found that models trained this way adapt to long context\n",
            "windows very slowly. After training for more than 10000 batches, the effective context window\n",
            "saw a minimal increase, moving from 2048 to 2560 (Table 4). This suggests that such method is\n",
            "inefficient for extending to substantially longer context windows.\n",
            "\n",
            "While certain techniques such as ALiBi (Press et al., 2022) and LeX (Sun et al., 2022) enable length\n",
            "extrapolation of Transformers, i.e. train on short context windows and inference on longer ones,\n",
            "many existing pre-trained LLMs, including LLaMA (Touvron et al., 2023), use positional encodings\n",
            "that have weak extrapolation properties (e.g., RoPE (Su et al., 2021)). Therefore, the applicability\n",
            "of these techniques for extending the context window sizes of such LLMs remains limited.\n",
            "\n",
            "In this work, we introduce Position Interpolation to enable context window extensions for certain\n",
            "existing pre-trained LLMs, including LLaMA. The key idea is, instead of extrapolation, we directly\n",
            "down-scale the position indices so that the maximum position index matches the previous context\n",
            "window limit in the pre-training stage. See Figure 1 for an illustration. In other words, to accom-\n",
            "modate more input tokens, we interpolate the position encodings at neighboring integer positions,\n",
            "utilizing the fact that position encodings can be applied on non-integer positions, as opposed to\n",
            "extrapolating outside the trained positions, which may lead to catastrophic values. We verify our\n",
            "approach theoretically, by showing that the interpolated attention score has a much smaller upper\n",
            "\n",
            "bound (~ 600x smaller in LLaMA 7B setting) than the extrapolated one, and is thus much more\n",
            "stable. Therefore, interpolated position encodings are easier for the model to adapt.\n",
            "\n",
            "Empirically, we found that Position Interpolation is highly effective and efficient, requiring only a\n",
            "very short period of fine-tuning for the model to fully adapt to greatly extended context windows.\n",
            "We present experimental results for extending the context window to up to 32768 from the initial\n",
            "2048 across 7B to 65B LLaMA models using Position Interpolation. Our results show that\n",
            "\n",
            "1. Position Interpolation can easily enable very long context windows (e.g. 32768), requiring\n",
            "only fine-tuning for 1000 steps on the Pile (Gao et al., 2020) to achieve a good quality.\n",
            "The cost of fine-tuning is negligible compared to the pre-training costs. This confirms\n",
            "our hypothesis that it is relatively easy for the models to adapt to interpolated position\n",
            "encodings.\n",
            "\n",
            "2. Position Interpolation generates strong models that can effectively make use of much ex-\n",
            "tended context window. We show that models extended by Position Interpolation enjoy\n",
            "significant perplexity gains from greatly extended context windows for text modeling, and\n",
            "we show that the perplexity reduces graceful with the enlargement of context windows.\n",
            "We also applied Position Interpolation in a long text summarization task, and demonstrate\n",
            "competitive performances.\n",
            "\n",
            "3. Position Interpolation preserves model quality relatively well for tasks within its original\n",
            "context window sizes. We present a variety of evaluation results for the extended LLaMA\n",
            "models on the original LLaMA benchmark. Compared with original LLaMA models, the\n",
            "extended LLLaM A models saw a minor degradation on several standard benchmarks within\n",
            "a 2048 token limit.\n",
            "\n",
            "Our results highlight the innate ability of Transformer models to “extrapolate to sequence lengths\n",
            "longer than the ones encountered during training” as hypothesized in the seminal work of Vaswani\n",
            "et al. (2017). We reaffirm this hypothesis and suggest that the previously known weakness of ex-\n",
            "trapolating to longer sequences for language modeling (Press et al., 2022) may be due to direct\n",
            "\n",
            "extrapolation of positional encodings and it can be largely mitigated by interpolating position en-\n",
            "codings instead.\n",
            "\n",
            "Concurrent work. Right before our release, we are informed with a concurrent blogpost (Super-\n",
            "HOT kaiokendev (2023)) that also interpolates positional encoding in RoPE to extend the context\n",
            "window from 2K to 8K. Recently, open source community picks it up in Reddit post ! and Github\n",
            "Issues 2, which shows that fine-tuning with LoRA (Hu et al., 2021) also seems to work well. Our\n",
            "paper shows a full fine-tuning with up to 65B model work well with Position Interpolation, and we\n",
            "also give theoretical explanations why interpolation achieves much more stable results than extrap-\n",
            "olation, by showing that the upper bound of interplated attention score is much lower than that of\n",
            "extrapolated ones.\n",
            "\n",
            "=== END OF FILE ===\n",
            "\n",
            "Question: What is the paper about?\n",
            "\n",
            "\n",
            "Answer: The paper is about extending the context window of LLaMA models.\n",
            "\n",
            "\n",
            "Question: What is the main idea?\n",
            "\n",
            "\n",
            "The main idea is to extend the context window of LLaMA models.\n",
            "\n",
            "\n",
            "Question: What is the main contribution?\n",
            "\n",
            "\n",
            "The main contribution is to extend the context window of LLaMA models.\n",
            "\n",
            "\n",
            "Question: What is the experimental setup?\n",
            "\n",
            "\n",
            "The experimental setup is to extend the context window\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(prompt + question, return_tensors=\"pt\").to(\"cuda\")\n",
        "print(inputs.input_ids.shape)\n",
        "gen_out = model.generate(**inputs, max_new_tokens = 100)\n",
        "print(tokenizer.batch_decode(gen_out)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ka9mB_RMqru8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "10b12751673740109b0f9c5b38c870db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "289f6bb171a64da68411830c7be7fb2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3139f4c7ad9341a9ad6c67ef796b57d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8c52246dfcf4434a1529e51a6298e49",
            "placeholder": "​",
            "style": "IPY_MODEL_560b16e6f4ec4f4c85cae88c4fac1f31",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "560b16e6f4ec4f4c85cae88c4fac1f31": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71b3e158d0764e9ba50dcbadbe736c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79409b5bc7574ad69eb57442d1a37fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3139f4c7ad9341a9ad6c67ef796b57d5",
              "IPY_MODEL_b74026602772438399311f1334c74481",
              "IPY_MODEL_c4a67ec132524d30b29a5118f40ce69d"
            ],
            "layout": "IPY_MODEL_fe016b72ea51457cae37e3059434e555"
          }
        },
        "857e276b44804fd09d43f8f71990ce06": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8c52246dfcf4434a1529e51a6298e49": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b74026602772438399311f1334c74481": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_857e276b44804fd09d43f8f71990ce06",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10b12751673740109b0f9c5b38c870db",
            "value": 2
          }
        },
        "c4a67ec132524d30b29a5118f40ce69d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_289f6bb171a64da68411830c7be7fb2d",
            "placeholder": "​",
            "style": "IPY_MODEL_71b3e158d0764e9ba50dcbadbe736c7c",
            "value": " 2/2 [01:07&lt;00:00, 30.71s/it]"
          }
        },
        "fe016b72ea51457cae37e3059434e555": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
